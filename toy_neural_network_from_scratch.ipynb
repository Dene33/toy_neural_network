{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toy neural network from scratch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dene33/toy_neural_network/blob/master/toy_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jTykv-UsauqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCYDLI3nbGl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "996f58e0-d9a3-4435-a1ce-8b93503acce5"
      },
      "cell_type": "code",
      "source": [
        "#initialization, x - features, y - predictions\n",
        "x = np.array([[3,5], [5,1], [10,2]], dtype=np.float64)\n",
        "y = np.array([[75], [82], [93]], dtype=np.float64)\n",
        "#normalization to make features and labels be in \"equal space\"\n",
        "x = x/np.max(x)\n",
        "y = y/100\n",
        "\n",
        "print('x(features):\\n{}\\n\\ny(predictions/labels):\\n{}'.format(x,y))"
      ],
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x(features):\n",
            "[[0.3 0.5]\n",
            " [0.5 0.1]\n",
            " [1.  0.2]]\n",
            "\n",
            "y(predictions/labels):\n",
            "[[0.75]\n",
            " [0.82]\n",
            " [0.93]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VLARH7HiaA5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network():\n",
        "  def __init__ (self, activation_func):\n",
        "    #layers of Neural Network with 2,3,1 neurons correspondingly\n",
        "    self.inputLayerSize = 2\n",
        "    self.hiddenLayerSize = 3\n",
        "    self.outputLayerSize = 1\n",
        "    #selecting of neurons' activation function\n",
        "    if activation_func == 'sigmoid':\n",
        "      self.activation_func = self.sigmoid\n",
        "    elif activation_func == 'RElu':\n",
        "      self.activation_func = self.RElu\n",
        "\n",
        "    #weights initialization, W1 - weights from input to hidden layer, W2 - from\n",
        "    #hidden to output layer\n",
        "    self.W1 = np.random.rand(self.inputLayerSize, self.hiddenLayerSize)\n",
        "    self.W2 = np.random.rand(self.hiddenLayerSize, self.outputLayerSize)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    #activation of neurons on hidden layer\n",
        "    self.a = self.neuron_activation(self.activation_func, x, self.W1)    \n",
        "    #activation of neurons on output layer\n",
        "    self.yPred = self.neuron_activation(self.activation_func, self.a, self.W2)\n",
        "    return self.yPred\n",
        "      \n",
        "  def gradient_descent(self, lr):    \n",
        "    #calculate gradients for W1 and W2 separately and update weights\n",
        "    self.W1 = self.gradient(x, self.W1, lr)\n",
        "    self.W2 = self.gradient(self.a, self.W2, lr)\n",
        "\n",
        "  def gradient(self, x, w, lr):\n",
        "    self.grad_y = self.neuron_activation(self.activation_func, x, w)\n",
        "    #gradient function with MSE-loss\n",
        "    return w - lr * (x.T @ ((self.grad_y - y) * self.derivative(self.grad_y, self.activation_func))) / len(y)\n",
        "\n",
        "  #features and weights multiplication  \n",
        "  def neuron_activation(self, activation, x, w):\n",
        "    return activation(np.dot(x,w))\n",
        "  \n",
        "  #derivative of sigmoid or RElu\n",
        "  def derivative(self, grad, activation_func):\n",
        "    if activation_func == self.sigmoid:\n",
        "      return grad * (1 - grad)\n",
        "    elif activation_func == self.RElu:\n",
        "      return 1. * np.array(grad > 0, dtype=float)\n",
        "  \n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  \n",
        "  def RElu(self, x):\n",
        "    return x * np.array(x > 0, dtype=float)\n",
        "  \n",
        "  def loss_MSE(self, yHat):\n",
        "    return 0.5 * np.mean((y-yHat)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OE3G07FZfLcN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$ sigmoid\\ gradient = \\frac{\\partial Loss}{\\partial w} = \\frac{1}{n} (\\sigma(w \\cdot X) - y)\\sigma(w \\cdot X)(1 - \\sigma(w \\cdot X))X$$"
      ]
    },
    {
      "metadata": {
        "id": "Q6dy3VH8jK4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ae30c461-a34d-4492-b111-b6c179984232"
      },
      "cell_type": "code",
      "source": [
        "test = Neural_Network('RElu')\n",
        "\n",
        "epoches = 10\n",
        "\n",
        "for epoch in range(epoches):\n",
        "  #forward pass\n",
        "  predictions = test.forward(x)\n",
        "  #adjust the weights\n",
        "  test.gradient_descent(lr=0.5)\n",
        "  print('Loss:',test.loss_MSE(test.yPred))\n"
      ],
      "execution_count": 498,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.1330625531552764\n",
            "Loss: 0.03682300379432412\n",
            "Loss: 0.02421460794332624\n",
            "Loss: 0.022459989240597153\n",
            "Loss: 0.021975192872028246\n",
            "Loss: 0.02166984756886205\n",
            "Loss: 0.02140534060918312\n",
            "Loss: 0.021158425023079696\n",
            "Loss: 0.020923712367384457\n",
            "Loss: 0.020699686854364626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYg-phXkFrXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f17937e4-faf8-4683-9bb6-6d5d25890591"
      },
      "cell_type": "code",
      "source": [
        "#verify the results\n",
        "print('Original labels:\\n',y)\n",
        "print('Predicted labels:\\n',test.yPred)"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original labels:\n",
            " [[0.75]\n",
            " [0.82]\n",
            " [0.93]]\n",
            "Predicted labels:\n",
            " [[0.6171966 ]\n",
            " [0.56988622]\n",
            " [1.13977244]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qt-gSObbozTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}